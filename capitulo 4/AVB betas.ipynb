{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AVB betas.ipynb","provenance":[],"authorship_tag":"ABX9TyPRyFY2komubaBMWgTiPS1t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"IQbQGD0kAdVM","executionInfo":{"status":"ok","timestamp":1627517882158,"user_tz":300,"elapsed":167,"user":{"displayName":"Ernesto Bustamante","photoUrl":"","userId":"05866849938369027248"}}},"source":["# AVB ejemplo betas\n","\n","import os\n","import pystan\n","import numpy as np\n","import scipy as sp\n","from matplotlib import pyplot as plt\n","from tensorflow.python.framework import ops\n","#pip install ite\n","import ite\n","from tqdm import tqdm_notebook\n","import re\n","import glob\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import time\n","import mpmath as mp\n","\n","\n","np.random.seed(145)\n","tf.random.set_seed(289612)\n","\n","\n","t_ini=time.time()\n","\n","n_a=40 # número de observaciones provenientes de theta A\n","n_b=40 # número de observaciones provenientes de theta B\n","theta = np.array([0.7,0.66])\n","x_a = np.random.binomial(1,theta[0],size=n_a) # observaciones provenientes de theta A\n","x_b = np.random.binomial(1,theta[1],size=n_b) # observaciones provenientes de theta B\n","\n","s_a = np.sum(x_a)\n","s_b = np.sum(x_b)\n","tam=100 # tamaño muestra aproximante\n","\n","# Parameters\n","batch_size = 512\n","data = {'J': 2, # dimensión de distribución posterior\n","        'n_a': n_a,\n","        'n_b': n_b,\n","        's_a': s_a,\n","        's_b': s_b,}\n","\n","\n","\n","param_dim = 2 # dimensión de parámetro\n","\n","def get_logprob(z, data):\n","    x1 = z[:, 0:1]\n","    x2 = z[:, 1:2]\n","\n","    n_a = tf.constant(data['n_a'], dtype=tf.float32, shape=(1, 1))\n","    n_b = tf.constant(data['n_b'], dtype=tf.float32, shape=(1, 1))\n","    s_a = tf.constant(data['s_a'], dtype=tf.float32, shape=(1, 1))\n","    s_b = tf.constant(data['s_b'], dtype=tf.float32, shape=(1, 1))\n","\n","    logprob = tf.reduce_sum(\n","        s_a*tf.math.log(x1) +(n_a-s_a)*tf.math.log(1-x1) + s_b*tf.math.log(x2) +(n_b-s_b)*tf.math.log(1-x2) , [1] ) # logaritmo de kernel verosimilitud\n","    \n","    return logprob\n","\n","\n","\n","red_posterior =  tf.keras.Sequential(\n","        [\n","         tf.keras.layers.Dense(128, activation=tf.nn.elu),\n","         tf.keras.layers.Dense(128, activation=tf.nn.elu),\n","         tf.keras.layers.Dense(param_dim, activation=tf.nn.sigmoid),\n","        ]\n","    )\n","\n","def posterior(num):\n","        eps = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros([num, param_dim]), scale_diag=tf.ones([num, param_dim])).sample()\n","        z=red_posterior(eps)\n","        return z\n","\n","red_adversaria =  tf.keras.Sequential(\n","        [\n","         tf.keras.layers.Dense(256, activation=tf.nn.elu),\n","         tf.keras.layers.Dense(256, activation=tf.nn.elu),\n","         tf.keras.layers.LeakyReLU(alpha=0.2),\n","         tf.keras.layers.Dense(1,activation=None,kernel_initializer=tf.keras.initializers.zeros()),\n","        ]\n","    )\n","\n","def adversary(z):\n","  T = red_adversaria(z)\n","  T = tf.squeeze(T, [1])\n","  return T\n","\n","def compute_loss():\n","  z0_a = tf.random.uniform(shape=(batch_size,1),dtype=tf.dtypes.float32)\n","  z0_b = tf.random.uniform(shape=(batch_size,1),dtype=tf.dtypes.float32, minval=0., maxval=z0_a)\n","  z0 = tf.concat([z0_a,z0_b],axis=1)\n","  z_ = posterior(batch_size)\n","  beta = tf.constant(1.)\n","\n","  Ti = adversary(z0)\n","  Td = adversary(z_)\n","\n","\n","  logprob = get_logprob(z_, data)\n","  mean_logprob = tf.reduce_mean(logprob)\n","  mean_Td = tf.reduce_mean(Td)\n","  loss_primal = tf.reduce_mean(beta*(Td) - logprob)\n","\n","  d_loss_d = tf.reduce_mean(\n","    tf.nn.sigmoid_cross_entropy_with_logits(logits=Td, labels=tf.ones_like(Td))\n","  )\n","  d_loss_i = tf.reduce_mean(\n","      tf.nn.sigmoid_cross_entropy_with_logits(logits=Ti, labels=tf.zeros_like(Ti))\n","  )\n","  loss_dual = d_loss_i + d_loss_d\n","\n","  return loss_primal, loss_dual\n","\n","optimizer_gen = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)\n","optimizer_disc = tf.keras.optimizers.Adam(1e-3, beta_1=0.5)\n","\n","@tf.function\n","def train_step(red_posterior, red_adversaria, optimizer_gen, optimizer_disc):\n","  \"\"\"Executes one training step and returns the loss.\n","\n","  This function computes the loss and gradients, and uses the latter to\n","  update the model's parameters.\n","  \"\"\"\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","    loss_aux = compute_loss()\n","    loss = loss_aux[0]\n","    disc_loss = loss_aux[1]\n","  gradients_gen = gen_tape.gradient(loss, red_posterior.trainable_variables)\n","  gradients_disc = disc_tape.gradient(disc_loss, red_adversaria.trainable_variables)\n","  optimizer_gen.apply_gradients(zip(gradients_gen, red_posterior.trainable_variables))\n","  optimizer_disc.apply_gradients(zip(gradients_disc, red_adversaria.trainable_variables))\n","\n","n_epoch = 5000\n","\n","\n","\n","vec_gen=np.array([])\n","vec_disc=np.array([])\n","for epoch in range(1, n_epoch + 1):\n","  start_time = time.time()\n","  train_step(red_posterior, red_adversaria, optimizer_gen, optimizer_disc)\n","  end_time = time.time()\n","\n","  aux_loss=compute_loss()\n","  gen_loss=aux_loss[0]\n","  disc_loss=aux_loss[1]\n","  vec_gen=np.concatenate([vec_gen,np.array([gen_loss])])\n","  vec_disc=np.concatenate([vec_disc,np.array([disc_loss])])\n","  \n","t_fin=time.time()\n","\n","def muestra(red_posterior,num):\n","  z_ = posterior(num)\n","  return z_\n","\n","plt.figure()\n","plt.plot(np.arange(0,n_epoch), np.asarray(vec_gen[0:]))\n","plt.xlabel('Epoch')\n","plt.ylabel('Pérdida red codificadora')\n","plt.show()\n","\n","plt.figure()\n","plt.plot(np.arange(0,n_epoch), np.asarray(vec_disc[0:]))\n","plt.xlabel('Epoch')\n","plt.ylabel('Pérdida red discriminativa')\n","plt.show()\n","\n","plt.figure()\n","plt.plot(np.arange(500,n_epoch), np.asarray(vec_gen[500:]))\n","plt.xlabel('Epoch')\n","plt.ylabel('Pérdida red codificadora')\n","plt.show()\n","\n","plt.figure()\n","plt.plot(np.arange(500,n_epoch), np.asarray(vec_disc[500:]))\n","plt.xlabel('Epoch')\n","plt.ylabel('Pérdida red discriminativa')\n","plt.show()\n","\n","t_ini_m=time.time()\n","n_vis = 1\n","enc_test = np.vstack([muestra(red_posterior,tam) for _ in range(n_vis)])\n","t_fin_m=time.time()\n","\n","l1=np.linspace(0,1,num=10)\n","\n","\n","def posterior_distr(a,b):\n","  if b>0 and b <=a and a<1:\n","    ff=s_a*np.log(a)+(n_a-s_a)*np.log(1-a)+s_b*np.log(b)+(n_b-s_b)*np.log(1-b)\n","    ff=np.exp(ff)\n","  else:\n","    ff=0.0\n","  return ff\n","\n","parta=125\n","partb=125\n","Z=np.zeros((parta,partb))\n","\n","ainf=0.0\n","asup=1.0\n","binf=0.0\n","bsup=1.0\n","\n","a=0\n","for i in np.linspace(ainf,asup,parta):\n","  b=0\n","  for j in np.linspace(binf,bsup,partb):\n","      Z[a,b]=posterior_distr(i,j)\n","      b=b+1\n","  a=a+1\n","\n","l1=np.linspace(0,1,num=10)\n","plt.contourf(np.linspace(ainf,asup,parta),np.linspace(binf,bsup,partb),np.transpose(Z),levels=20)\n","plt.colorbar().ax.set_ylabel('densidad no normalizada')\n","plt.scatter(enc_test[0:tam,0], enc_test[0:tam,1],marker='o',color=\"orange\",facecolors='none',label='Muestra',linewidth=1.5)\n","#plt.scatter(theta[0],theta[1],facecolors=\"r\")\n","#plt.plot(l1,l1,color=\"r\")\n","plt.show()\n","\n","media=np.mean(enc_test,0)\n","\n","print('Segundos: ', np.round(t_fin-t_ini,decimals=2))\n","print('Minutos: ', np.round((t_fin-t_ini)/60.,decimals=2))\n","print(\"\")\n","print('Segundos muestreo: ', np.round(t_fin_m-t_ini_m,decimals=6))\n","print(\"\")\n","print(\"media: \",media,sep=\"\")\n","print(\"\")\n","print(\"Reales\")\n","print(\"medias: \",theta,sep=\"\")"],"execution_count":4,"outputs":[]}]}